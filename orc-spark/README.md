# ORC Spark Project

## Overview

This project demonstrates the use of Apache Spark and Apache Iceberg for processing, storing, and querying data in ORC (Optimized Row Columnar) format. It provides a ready-to-use environment with Docker Compose, Jupyter notebooks for data processing and analysis, and a sample dataset for experimentation.

## Project Structure

- `docker-compose.yml`: Defines services for Spark with Iceberg and a Jupyter notebook environment.
- `notebooks/`: Contains Jupyter notebooks and sample data.
  - `bloomfilters.ipynb`: Example notebook showing how to write ORC files with Bloom filters using PySpark.
  - `convertcsv.ipynb`: (Large file) Likely demonstrates CSV to ORC conversion or related data processing.
  - `stores.csv`: Sample dataset of store locations (store_id, store_name, latitude, longitude).
- `warehouse/`: Data warehouse directory for storing ORC and metadata files.
  - `db/` and `db2/`: Example Iceberg/Spark databases with ORC data and metadata.

## Getting Started

### Prerequisites
- Docker
- Docker Compose

### Setup
1. Clone this repository.
2. Navigate to the `orc-spark` directory.
3. Start the services:
   ```sh
   docker-compose up -d
   ```
4. Access Jupyter Notebook at [http://localhost:8899](http://localhost:8899) (no token or password required).

### Notebooks
- **bloomfilters.ipynb**: Shows how to load a CSV, write it as ORC with Bloom filters, and run SQL queries using Spark.
- **convertcsv.ipynb**: (File too large to preview) Presumably for converting CSV data to ORC or other data engineering tasks.

### Data
- **stores.csv**: Sample data with columns such as `store_id`, `store_name`, `latitude`, and `longitude`.
- **warehouse/db2/stores_orc/**: Contains ORC files generated by the notebooks.

## Services
- **spark-iceberg**: Spark with Iceberg support for advanced table management and analytics.
- **jupyter**: Jupyter notebook server with PySpark and Iceberg dependencies.

## Example: Writing ORC with Bloom Filters
The `bloomfilters.ipynb` notebook demonstrates:
- Loading CSV data into Spark DataFrame
- Writing ORC files with Bloom filters on specific columns
- Querying the resulting ORC data using Spark SQL

## License
This project is for educational and demonstration purposes. 